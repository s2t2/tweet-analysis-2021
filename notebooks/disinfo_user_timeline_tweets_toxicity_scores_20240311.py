# -*- coding: utf-8 -*-
"""Disinfo User Timeline Tweets - Toxicity Scores - 20240311

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ERLM3_YzzDNZVwFcnk5r_hXQtKxUFlP6

Using a dataset of "original" timeline tweets (excludes rt, reply, quotes) by Q-anon supporters, we obtain toxicity scores using the Detoxify model, and store the scores in a CSV file on drive for further analysis.

## Setup

Package installations:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install detoxify

!pip list | grep detoxify #> 0.5.2

!pip list | grep torch #> 2.1.0+cu121

"""### Google Drive"""

import os
from google.colab import drive

drive.mount('/content/drive')

# you might need to create a google drive SHORTCUT that has this same path
# ... or update the path to use your own google drive organization
DIRPATH = '/content/drive/MyDrive/Research/DS Research Shared 2024'
os.path.isdir(DIRPATH)

DATA_DIRPATH = os.path.join(DIRPATH, "projects", "Disinformation 2021 Embeddings", "data")
os.path.isdir(DATA_DIRPATH)

"""## Load Dataset"""

from pandas import read_csv

csv_filepath = os.path.join(DATA_DIRPATH, "q_user_timeline_tweets_original.csv.gz")

df = read_csv(csv_filepath, compression="gzip")
print(df.shape)
print(df.columns.tolist())
df.head()

print(len(df))
df["status_id"].nunique()

df["status_text"].nunique()

"""We need to fetch scores for around 850K tweets.

### Dataset Exploration
"""

from pandas import to_datetime

df["created_at"] = to_datetime(df["created_at"])
df["lookup_at"] = to_datetime(df["lookup_at"])

df["created_on"] = df["created_at"].dt.date
df["lookup_on"] = df["lookup_at"].dt.date

df.info()

import plotly.express as px
from datetime import date


title = "Qanon User Timeline Tweets"
title += f"<br><sup>Collected from 2/10 - 5/15 2021</sup>"

chart_df = df[df["created_on"] >= date(2020, 1, 1)]
px.histogram(chart_df, x="created_on", title=title)

#df.groupby("created_on").agg(
#    user_count=("user_id", "nunique"),
#    status_count=("status_id", "nunique"),
#).reset_index()

import plotly.express as px
from datetime import date

chart_df = df[df["created_on"] >= date(2020, 1, 1)]
chart_df = chart_df.groupby("created_on").agg(
    user_count=("user_id", "nunique"),
    status_count=("status_id", "nunique"),
).reset_index()

title = "Active Qanon Users by Day"
title += f"<br><sup>Collected from 2/10 - 5/15 2021</sup>"
px.bar(chart_df, x="created_on", y="user_count", title=title)

#import plotly.express as px
#from datetime import date

title = "Qanon Statuses by Day"
title += f"<br><sup>Collected from 2/10 - 5/15 2021</sup>"

px.bar(chart_df, x="created_on", y="status_count", title=title)



"""## Detoxify Model

We want to use an approach that is similar to our previous approach:

  + https://github.com/s2t2/tweet-analysis-2020/tree/main/app/toxicity
  + https://github.com/s2t2/tweet-analysis-2020/blob/main/app/toxicity/checkpoint_scorer.py
  + https://github.com/s2t2/tweet-analysis-2020/blob/main/app/toxicity/checkpoint_scorer_async.py

Previous methods needed to load the model from checkpoints, in a somewhat complex way, to satisfy limitations of getting the model to download onto a server. This limitation is no longer in place if we are running in Colab.

We can use a transformer pipeline approach, but to be more consistent with previous methods, we will use the detoxify package directly.
"""

#from google.colab import userdata
#token = userdata.get('HF_TOKEN')
#print(token[0:3], "...")

#from transformers import pipeline
#
## https://github.com/unitaryai/detoxify/issues/15
## https://huggingface.co/transformers/v4.11.3/_modules/transformers/pipelines/text_classification.html
#
#detoxify_pipeline = pipeline('text-classification',
#    model='unitary/toxic-bert',
#    tokenizer='bert-base-uncased',
#    # function_to_apply='sigmoid', # "default"`: if the model has a single label, will apply the sigmoid function on the output. If the model has several labels, will apply the softmax function on the output.
#    #return_all_scores=True # `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True`
#    top_k=None #
#)

#detoxify_pipeline('shut up, you idiot!')

"""Compare against detoxify package:"""

from detoxify import Detoxify

model = Detoxify("original")

results = model.predict(["shut up, you idiot!", "please lower your voice"])
results

"""### Example Texts"""

texts = [
    "RT @realDonaldTrump: Crazy Nancy Pelosi should spend more time in her decaying city and less time on the Impeachment Hoax! https://t.co/eno…",
    "RT @SpeakerPelosi: The House cannot choose our impeachment managers until we know what sort of trial the Senate will conduct. President Tr…",
]

scores = model.predict(texts)
scores



#scores = detoxify_pipeline(texts)
#scores

#from pandas import DataFrame
#
#scores = model.predict(texts)
#
#scores_df = DataFrame(scores)
#scores_df["text"] = texts
#scores_df

#import torch
#from detoxify import Detoxify
#from pandas import DataFrame
#
#DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
#
#class Job:
#    def __init__(self, model_name="original", device=DEVICE):
#        self.device = device
#        self.model = Detoxify(model_type=model_name, device=device)
#
#    def preds_df(self, texts):
#        scores = self.model.predict(texts)
#        scores_df = DataFrame(scores)
#        scores_df["text"] = texts
#        return scores_df
#

#texts

#job = Job()
#
#job.preds_df(texts)

#%%timeit
#
## 37 seconds for 100 texts (CPU)
## 2.44 seconds for 1000 texts (GPU)
#
#batch = df["status_text"][0:1000].tolist()
#scores_batch = job.preds_df(batch)
#scores_batch

"""GPU to the rescue. Let's roll."""

#len(df["status_text"].tolist())

"""Job estimated completion time for 800K texts is around 30 mins."""

#scores_df = job.preds_df(df["status_text"].tolist()) # # OutOfMemoryError: CUDA out of memory. Tried to allocate 872.96 GiB. GPU 0 has a total capacty of 15.77 GiB of which 6.99 GiB is free. Process 5197 has 8.78 GiB memory in use. Of the allocated memory 8.37 GiB is allocated by PyTorch, and 45.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
#scores_df

"""Ran out of memory doing the whole dataset at once. Let's process in batches.

## Job
"""

def split_into_batches(my_list, batch_size=9000):
    """Splits a list into evenly sized batches"""
    # h/t: h/t: https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks
    for i in range(0, len(my_list), batch_size):
        yield my_list[i : i + batch_size]


example_batches = split_into_batches([0,1,2,3,4,5,6,7,8,9,10], 3)
assert list(example_batches) == [
    [0, 1, 2],
    [3, 4, 5],
    [6, 7, 8],
    [9, 10]
]

# ignore warnings:
import warnings
warnings.filterwarnings("ignore")

import torch
from detoxify import Detoxify
from pandas import DataFrame, concat
import gc

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 500

class ToxicityScorer:
    def __init__(self, model_name="original", device=DEVICE, batch_size=BATCH_SIZE):
        self.model_name = model_name
        self.device = device
        self.model = Detoxify(model_type=self.model_name, device=self.device)
        print(self.model_name, self.device, self.model.class_names)

        self.batch_size = batch_size
        self.batch_counter = 0

        self.preds_df = DataFrame()

    def predict(self, texts):
        scores = self.model.predict(texts)
        scores_df = DataFrame(scores)
        scores_df["text"] = texts
        return scores_df

    def process_in_batches(self, texts):
        batches = split_into_batches(texts, batch_size=self.batch_size)

        print(f"SCORING TEXTS IN BATCHES OF {self.batch_size}")
        for batch in batches:
            preds_batch = self.predict(batch)
            self.preds_df = self.preds_df.append(preds_batch)
            #self.preds_df = concat(self.preds_df, preds_batch)

            self.batch_counter += 1
            print(f"... BATCH {self.batch_counter} -- {len(batch)} -- {len(self.preds_df)}")

            # memory
            torch.cuda.empty_cache()
            del preds_batch
            gc.collect()

gc.collect()

job = ToxicityScorer()
job.process_in_batches(texts)
job.preds_df.head()

#batches = split_into_batches(df["status_text"].tolist(), batch_size=5_000)

job = ToxicityScorer()
job.process_in_batches(df["status_text"].tolist())
job.preds_df.head()



"""## Saving Scores"""

scores_csv_filepath = os.path.join(DATA_DIRPATH, "q_user_timeline_tweets_detoxify_20240311.csv.gz")
job.preds_df.to_csv(scores_csv_filepath, index=False, compression="gzip")

job.preds_df.index = df.index
scores_df = df.merge(job.preds_df, left_index=True, right_index=True)
scores_df

scores_df.to_csv(scores_csv_filepath, index=False, compression="gzip")





